{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e234ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Provide the full paths to the .cfg and .weights files\n",
    "config_path = \"C:/Users/pooja/Downloads/yolov3.cfg\"\n",
    "weights_path = \"C:/Users/pooja/Downloads/yolov3.weights\"\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet(weights_path, config_path)\n",
    "\n",
    "# Continue with your object detection code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c01d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pooja\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "112a2e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\pooja\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\pooja\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\pooja\\anaconda3\\Lib\\site-packages\\torch-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\pooja\\anaconda3\\Lib\\site-packages\\torch-2.2.2.dist-info due to invalid metadata entry 'name'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281abcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Names: ('conv_0', 'bn_0', 'leaky_1', 'conv_1', 'bn_1', 'leaky_2', 'conv_2', 'bn_2', 'leaky_3', 'conv_3', 'bn_3', 'leaky_4', 'shortcut_4', 'conv_5', 'bn_5', 'leaky_6', 'conv_6', 'bn_6', 'leaky_7', 'conv_7', 'bn_7', 'leaky_8', 'shortcut_8', 'conv_9', 'bn_9', 'leaky_10', 'conv_10', 'bn_10', 'leaky_11', 'shortcut_11', 'conv_12', 'bn_12', 'leaky_13', 'conv_13', 'bn_13', 'leaky_14', 'conv_14', 'bn_14', 'leaky_15', 'shortcut_15', 'conv_16', 'bn_16', 'leaky_17', 'conv_17', 'bn_17', 'leaky_18', 'shortcut_18', 'conv_19', 'bn_19', 'leaky_20', 'conv_20', 'bn_20', 'leaky_21', 'shortcut_21', 'conv_22', 'bn_22', 'leaky_23', 'conv_23', 'bn_23', 'leaky_24', 'shortcut_24', 'conv_25', 'bn_25', 'leaky_26', 'conv_26', 'bn_26', 'leaky_27', 'shortcut_27', 'conv_28', 'bn_28', 'leaky_29', 'conv_29', 'bn_29', 'leaky_30', 'shortcut_30', 'conv_31', 'bn_31', 'leaky_32', 'conv_32', 'bn_32', 'leaky_33', 'shortcut_33', 'conv_34', 'bn_34', 'leaky_35', 'conv_35', 'bn_35', 'leaky_36', 'shortcut_36', 'conv_37', 'bn_37', 'leaky_38', 'conv_38', 'bn_38', 'leaky_39', 'conv_39', 'bn_39', 'leaky_40', 'shortcut_40', 'conv_41', 'bn_41', 'leaky_42', 'conv_42', 'bn_42', 'leaky_43', 'shortcut_43', 'conv_44', 'bn_44', 'leaky_45', 'conv_45', 'bn_45', 'leaky_46', 'shortcut_46', 'conv_47', 'bn_47', 'leaky_48', 'conv_48', 'bn_48', 'leaky_49', 'shortcut_49', 'conv_50', 'bn_50', 'leaky_51', 'conv_51', 'bn_51', 'leaky_52', 'shortcut_52', 'conv_53', 'bn_53', 'leaky_54', 'conv_54', 'bn_54', 'leaky_55', 'shortcut_55', 'conv_56', 'bn_56', 'leaky_57', 'conv_57', 'bn_57', 'leaky_58', 'shortcut_58', 'conv_59', 'bn_59', 'leaky_60', 'conv_60', 'bn_60', 'leaky_61', 'shortcut_61', 'conv_62', 'bn_62', 'leaky_63', 'conv_63', 'bn_63', 'leaky_64', 'conv_64', 'bn_64', 'leaky_65', 'shortcut_65', 'conv_66', 'bn_66', 'leaky_67', 'conv_67', 'bn_67', 'leaky_68', 'shortcut_68', 'conv_69', 'bn_69', 'leaky_70', 'conv_70', 'bn_70', 'leaky_71', 'shortcut_71', 'conv_72', 'bn_72', 'leaky_73', 'conv_73', 'bn_73', 'leaky_74', 'shortcut_74', 'conv_75', 'bn_75', 'leaky_76', 'conv_76', 'bn_76', 'leaky_77', 'conv_77', 'bn_77', 'leaky_78', 'conv_78', 'bn_78', 'leaky_79', 'conv_79', 'bn_79', 'leaky_80', 'conv_80', 'bn_80', 'leaky_81', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'leaky_85', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'leaky_88', 'conv_88', 'bn_88', 'leaky_89', 'conv_89', 'bn_89', 'leaky_90', 'conv_90', 'bn_90', 'leaky_91', 'conv_91', 'bn_91', 'leaky_92', 'conv_92', 'bn_92', 'leaky_93', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'leaky_97', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'leaky_100', 'conv_100', 'bn_100', 'leaky_101', 'conv_101', 'bn_101', 'leaky_102', 'conv_102', 'bn_102', 'leaky_103', 'conv_103', 'bn_103', 'leaky_104', 'conv_104', 'bn_104', 'leaky_105', 'conv_105', 'permute_106', 'yolo_106')\n",
      "Output Layers: [200 227 254]\n"
     ]
    }
   ],
   "source": [
    "layer_names = net.getLayerNames()\n",
    "output_layers = net.getUnconnectedOutLayers()\n",
    "print(\"Layer Names:\", layer_names)\n",
    "print(\"Output Layers:\", output_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b20681ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Step 1: Load YOLO Model\n",
    "config_path = \"C:/Users/pooja/Downloads/yolov3.cfg\"\n",
    "weights_path = \"C:/Users/pooja/Downloads/yolov3.weights\"\n",
    "net = cv2.dnn.readNet(weights_path, config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f332d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Image\n",
    "image_path = \"C:/Users/pooja/Downloads/image.jpg\"  # Replace with your image path\n",
    "img = cv2.imread(image_path)\n",
    "height, width, _ = img.shape  # Get image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0333ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare Image for YOLO\n",
    "blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "net.setInput(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0b0575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Get Output Layer Names\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]  # Adjust the indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56b873f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Run Forward Pass to Get Predictions\n",
    "outputs = net.forward(output_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5b4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Initialize Lists for Bounding Boxes and Confidences\n",
    "boxes = []\n",
    "confidences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35bf28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Process the Outputs\n",
    "for output in outputs:\n",
    "    for detection in output:\n",
    "        scores = detection[5:]  # Skip the first 5 values, which are box parameters\n",
    "        class_id = np.argmax(scores)  # Get the index of the highest score\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.3:  # Lower confidence threshold to detect more objects\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b936f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Draw Bounding Boxes on the Image\n",
    "for i in range(len(boxes)):\n",
    "    x, y, w, h = boxes[i]\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.putText(img, \"Object detected\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce4f5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Display the Resulting Image\n",
    "cv2.imshow(\"Image\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ede00b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Object Dimensions: Width = 10.00 cm, Height = 12.82 cm\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Dimension Estimation (Optional)\n",
    "if len(boxes) > 0:\n",
    "    # Use the dimensions of the first detected object (or the largest one if needed)\n",
    "    first_box = boxes[0]  # Get the first detected object's bounding box\n",
    "    w = first_box[2]  # width of the object\n",
    "    h = first_box[3]  # height of the object\n",
    "\n",
    "    ref_width = 10  # cm (the known width of a reference object)\n",
    "    pixel_per_metric = w / ref_width  # Calculate pixels per centimeter\n",
    "    real_width = w / pixel_per_metric\n",
    "    real_height = h / pixel_per_metric\n",
    "\n",
    "    print(f\"Estimated Object Dimensions: Width = {real_width:.2f} cm, Height = {real_height:.2f} cm\")\n",
    "else:\n",
    "    print(\"No objects detected to estimate dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b00edfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested box size: {'length': 20, 'width': 15, 'height': 10}\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Suggest Box Size\n",
    "box_sizes = [\n",
    "    {\"length\": 15, \"width\": 10, \"height\": 5},\n",
    "    {\"length\": 20, \"width\": 15, \"height\": 10},\n",
    "    {\"length\": 30, \"width\": 20, \"height\": 15},\n",
    "    # Add more box sizes as needed\n",
    "]\n",
    "\n",
    "def suggest_box_size(length, width, height):\n",
    "    for box in box_sizes:\n",
    "        if length <= box[\"length\"] and width <= box[\"width\"] and height <= box[\"height\"]:\n",
    "            return box\n",
    "    return None\n",
    "\n",
    "# Suggest the box size based on estimated dimensions\n",
    "if len(boxes) > 0:\n",
    "    box = suggest_box_size(real_width, real_height, 5)  # Assuming an object height of 5 cm\n",
    "    if box:\n",
    "        print(f\"Suggested box size: {box}\")\n",
    "    else:\n",
    "        print(\"No suitable box size found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a0ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
